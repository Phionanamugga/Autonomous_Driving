{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90be8ebc",
   "metadata": {},
   "source": [
    "Problem Statement: End-to-End Autonomous Driving Pipeline\n",
    "Context\n",
    "The advancement of autonomous vehicles promises to revolutionize transportation by enhancing safety, efficiency, and accessibility. However, developing a reliable autonomous driving system requires integrating complex subsystems for perception, planning, and control, all while ensuring real-time performance and robustness in dynamic environments.\n",
    "Problem\n",
    "The primary challenge is to design and implement an end-to-end pipeline for autonomous driving that enables a vehicle to navigate safely from a starting point to a destination in a simulated urban environment. The system must:\n",
    "\n",
    "Accurately perceive the environment using sensor data (e.g., camera and LiDAR) to detect objects and obstacles.\n",
    "Plan a safe and efficient path, accounting for static and dynamic obstacles while adhering to traffic rules.\n",
    "Execute precise vehicle control to follow the planned path, maintaining stability and passenger comfort.\n",
    "Operate in real-time with minimal latency to respond to changing conditions.\n",
    "\n",
    "Objectives\n",
    "\n",
    "Develop a modular pipeline that integrates sensor data acquisition, perception, path planning, and vehicle control.\n",
    "Ensure the system can detect and classify objects (e.g., vehicles, pedestrians) with high accuracy using camera-based object detection.\n",
    "Identify obstacles in 3D space using LiDAR data to enhance environmental understanding.\n",
    "Generate a collision-free path that aligns with a global route and adapts to local obstacles.\n",
    "Implement a control mechanism to smoothly execute the planned path with appropriate speed and steering.\n",
    "Validate the pipeline in a simulated environment (e.g., CARLA) to demonstrate functionality and robustness.\n",
    "\n",
    "Constraints\n",
    "\n",
    "The pipeline must process sensor data and generate control commands within a 100ms cycle to ensure real-time performance.\n",
    "The system should operate within the computational limits of a standard GPU-enabled platform (e.g., NVIDIA RTX 3080).\n",
    "The solution must prioritize safety by avoiding collisions and adhering to speed limits (e.g., 30 km/h in urban settings).\n",
    "The pipeline should be developed using open-source tools and libraries (e.g., Python, OpenCV, YOLOv5, Open3D) to ensure reproducibility.\n",
    "\n",
    "Success Criteria\n",
    "\n",
    "The vehicle successfully navigates a predefined route in a simulated urban environment without collisions.\n",
    "Object detection achieves at least 90% precision and recall for critical classes (e.g., vehicles, pedestrians).\n",
    "The system maintains a maximum deviation of 0.5 meters from the planned path during navigation.\n",
    "Control commands ensure smooth operation with acceleration/deceleration within ±2 m/s² for passenger comfort.\n",
    "The pipeline operates reliably for at least 10 minutes of continuous driving in varied simulated conditions (e.g., traffic, obstacles).\n",
    "\n",
    "Scope\n",
    "This problem focuses on a simulated environment to demonstrate core autonomous driving capabilities. Real-world deployment considerations, such as hardware redundancy, regulatory compliance, and extreme weather handling, are out of scope but noted for future extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddc15d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "from typing import List, Tuple\n",
    "import carla\n",
    "import torch\n",
    "from yolov5 import YOLOv5  # Assuming YOLOv5 for object detection\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cd72a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    SENSOR_TICK = 0.1  # Sensor update frequency (seconds)\n",
    "    VEHICLE_SPEED_LIMIT = 30.0  # km/h\n",
    "    LIDAR_RANGE = 50.0  # meters\n",
    "    CAMERA_FOV = 90.0  # degrees\n",
    "    WAYPOINT_SPACING = 2.0  # meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0bab36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sensor Data Acquisition\n",
    "class SensorSuite:\n",
    "    def __init__(self, vehicle: carla.Vehicle, world: carla.World):\n",
    "        self.vehicle = vehicle\n",
    "        self.world = world\n",
    "        self.camera = None\n",
    "        self.lidar = None\n",
    "        self.setup_sensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0678458",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def setup_sensors(self):\n",
    "        # Camera setup\n",
    "        camera_bp = self.world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "        camera_bp.set_attribute('image_size_x', '1920')\n",
    "        camera_bp.set_attribute('image_size_y', '1080')\n",
    "        camera_bp.set_attribute('fov', str(Config.CAMERA_FOV))\n",
    "        camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\n",
    "        self.camera = self.world.spawn_actor(camera_bp, camera_transform, attach_to=self.vehicle)\n",
    "        self.camera.listen(lambda image: self.process_camera(image))\n",
    "\n",
    "        # LiDAR setup\n",
    "        lidar_bp = self.world.get_blueprint_library().find('sensor.lidar.ray_cast')\n",
    "        lidar_bp.set_attribute('range', str(Config.LIDAR_RANGE))\n",
    "        lidar_transform = carla.Transform(carla.Location(x=1.5, z=2.4))\n",
    "        self.lidar = self.world.spawn_actor(lidar_bp, lidar_transform, attach_to=self.vehicle)\n",
    "        self.lidar.listen(lambda point_cloud: self.process_lidar(point_cloud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214efc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_camera(self, image):\n",
    "        # Convert CARLA image to OpenCV format\n",
    "        array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "        array = array.reshape((image.height, image.width, 4))\n",
    "        self.latest_camera = cv2.cvtColor(array, cv2.COLOR_BGRA2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f9136",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_lidar(self, point_cloud):\n",
    "        # Convert CARLA point cloud to Open3D format\n",
    "        points = np.array([[p.x, p.y, p.z] for p in point_cloud])\n",
    "        self.latest_lidar = o3d.geometry.PointCloud()\n",
    "        self.latest_lidar.points = o3d.utility.Vector3dVector(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a6813",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Perception Module\n",
    "class Perception:\n",
    "    def __init__(self):\n",
    "        # Initialize YOLOv5 model for object detection\n",
    "        self.model = YOLOv5('yolov5s.pt', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.objects = []\n",
    "\n",
    "    def detect_objects(self, image: np.ndarray) -> List[dict]:\n",
    "        # Run YOLOv5 inference\n",
    "        results = self.model.predict(image)\n",
    "        self.objects = []\n",
    "        for det in results.xyxy[0]:  # [x1, y1, x2, y2, conf, cls]\n",
    "            x1, y1, x2, y2, conf, cls = det\n",
    "            if conf > 0.5:  # Confidence threshold\n",
    "                self.objects.append({\n",
    "                    'class': results.names[int(cls)],\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'confidence': conf\n",
    "                })\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c2f49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_lidar(self, point_cloud: o3d.geometry.PointCloud) -> List[Tuple[float, float, float]]:\n",
    "        # Cluster points to detect obstacles\n",
    "        labels = np.array(point_cloud.cluster_dbscan(eps=0.5, min_points=10))\n",
    "        obstacles = []\n",
    "        for label in np.unique(labels):\n",
    "            if label != -1:  # Ignore noise\n",
    "                cluster_points = np.asarray(point_cloud.points)[labels == label]\n",
    "                centroid = np.mean(cluster_points, axis=0)\n",
    "                obstacles.append(tuple(centroid))\n",
    "        return obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bdd33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Path Planning\n",
    "class PathPlanner:\n",
    "    def __init__(self, world: carla.World):\n",
    "        self.world = world\n",
    "        self.map = world.get_map()\n",
    "        self.global_path = []\n",
    "\n",
    "    def generate_global_path(self, start: carla.Location, end: carla.Location):\n",
    "        # Use CARLA's global planner\n",
    "        start_waypoint = self.map.get_waypoint(start)\n",
    "        end_waypoint = self.map.get_waypoint(end)\n",
    "        self.global_path = start_waypoint.get_route_to(end_waypoint, spacing=Config.WAYPOINT_SPACING)\n",
    "        return self.global_path\n",
    "\n",
    "    def local_path(self, current_pose: carla.Transform, obstacles: List[Tuple[float, float, float]]) -> List[carla.Waypoint]:\n",
    "        # Simple local planner: Follow global path, avoid obstacles\n",
    "        local_path = []\n",
    "        for waypoint in self.global_path[:10]:  # Look ahead 10 waypoints\n",
    "            safe = True\n",
    "            for obs in obstacles:\n",
    "                dist = np.sqrt((waypoint.transform.location.x - obs[0])**2 + (waypoint.transform.location.y - obs[1])**2)\n",
    "                if dist < 2.0:  # Obstacle too close\n",
    "                    safe = False\n",
    "                    break\n",
    "            if safe:\n",
    "                local_path.append(waypoint)\n",
    "        return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6220f92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vehicle Control\n",
    "class Controller:\n",
    "    def __init__(self, vehicle: carla.Vehicle):\n",
    "        self.vehicle = vehicle\n",
    "        self.target_speed = Config.VEHICLE_SPEED_LIMIT / 3.6  # Convert km/h to m/s\n",
    "\n",
    "    def compute_control(self, current_pose: carla.Transform, target_waypoint: carla.Waypoint) -> carla.VehicleControl:\n",
    "        # Simple PID controller for throttle and steering\n",
    "        control = carla.VehicleControl()\n",
    "        \n",
    "        # Steering\n",
    "        target_x, target_y = target_waypoint.transform.location.x, target_waypoint.transform.location.y\n",
    "        current_x, current_y = current_pose.location.x, current_pose.location.y\n",
    "        angle = np.arctan2(target_y - current_y, target_x - current_x)\n",
    "        current_yaw = np.deg2rad(current_pose.rotation.yaw)\n",
    "        steering = np.clip(angle - current_yaw, -1.0, 1.0)\n",
    "        control.steer = steering\n",
    "\n",
    "        # Throttle\n",
    "        current_speed = np.sqrt(self.vehicle.get_velocity().x**2 + self.vehicle.get_velocity().y**2)\n",
    "        throttle = 0.5 * (self.target_speed - current_speed)  # Simple proportional control\n",
    "        control.throttle = np.clip(throttle, 0.0, 1.0)\n",
    "        control.brake = 0.0 if throttle > 0 else 0.5\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faddd49a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main Pipeline\n",
    "class AutonomousDrivingPipeline:\n",
    "    def __init__(self, host='localhost', port=2000):\n",
    "        self.client = carla.Client(host, port)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.vehicle = None\n",
    "        self.sensors = None\n",
    "        self.perception = Perception()\n",
    "        self.planner = PathPlanner(self.world)\n",
    "        self.controller = None\n",
    "\n",
    "    def initialize(self):\n",
    "        # Spawn vehicle\n",
    "        blueprint_library = self.world.get_blueprint_library()\n",
    "        vehicle_bp = blueprint_library.filter('model3')[0]\n",
    "        spawn_point = self.world.get_map().get_spawn_points()[0]\n",
    "        self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        self.sensors = SensorSuite(self.vehicle, self.world)\n",
    "        self.controller = Controller(self.vehicle)\n",
    "\n",
    "        # Set destination and generate global path\n",
    "        destination = carla.Location(x=100, y=100, z=0)  # Example destination\n",
    "        self.planner.generate_global_path(self.vehicle.get_location(), destination)\n",
    "\n",
    "    def run_step(self):\n",
    "        # Perception\n",
    "        if hasattr(self.sensors, 'latest_camera'):\n",
    "            objects = self.perception.detect_objects(self.sensors.latest_camera)\n",
    "        else:\n",
    "            objects = []\n",
    "        if hasattr(self.sensors, 'latest_lidar'):\n",
    "            obstacles = self.perception.process_lidar(self.sensors.latest_lidar)\n",
    "        else:\n",
    "            obstacles = []\n",
    "\n",
    "        # Planning\n",
    "        local_path = self.planner.local_path(self.vehicle.get_transform(), obstacles)\n",
    "        if not local_path:\n",
    "            return  # Stop if no safe path\n",
    "\n",
    "        # Control\n",
    "        control = self.controller.compute_control(self.vehicle.get_transform(), local_path[0])\n",
    "        self.vehicle.apply_control(control)\n",
    "\n",
    "    def run(self):\n",
    "        self.initialize()\n",
    "        try:\n",
    "            while True:\n",
    "                self.run_step()\n",
    "                time.sleep(Config.SENSOR_TICK)\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if self.sensors.camera:\n",
    "                self.sensors.camera.destroy()\n",
    "            if self.sensors.lidar:\n",
    "                self.sensors.lidar.destroy()\n",
    "            if self.vehicle:\n",
    "                self.vehicle.destroy()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = AutonomousDrivingPipeline()\n",
    "    pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
